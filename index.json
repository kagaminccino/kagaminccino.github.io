[{"authors":["admin"],"categories":null,"content":"Shang-Yi Chuang is a research assistant working on artificial intelligence at the Biomedical Acoustic Signal Processing Lab, Academia Sinica, Taiwan. Her research interests include natural language processing, speech processing, and computer vision. She has collaborated with Prof. Yu Tsao and Prof. Hsin-Min Wang on audio-visual multimodal learning and data compression on speech enhancement (SE). Currently she is engaged in a project of a cross-lingual question answering (QA) system with Prof. Keh-Yih Su.\nBefore she went to Academia Sinica, she was conducting research about humanoid robots with Prof. Tomomichi Sugihara at Motor Intelligence, Osaka University. Her work was to improve the control system of a robot arm in order to create a safer and more comfortable human-robot coworking space.\n","date":1606003200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1606003200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://kagaminccino.github.io/author/shang-yi-chuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shang-yi-chuang/","section":"authors","summary":"Shang-Yi Chuang is a research assistant working on artificial intelligence at the Biomedical Acoustic Signal Processing Lab, Academia Sinica, Taiwan. Her research interests include natural language processing, speech processing, and computer vision.","tags":null,"title":"Shang-Yi Chuang","type":"authors"},{"authors":["Yu-Wen Chen","Kuo-Hsuan Hung","Shang-Yi Chuang","Jonathan Sherman","Xugang Lu","Yu Tsao"],"categories":null,"content":"","date":1606003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606003200,"objectID":"a12991d861022c7bb67065cd83c16c44","permalink":"https://kagaminccino.github.io/publication/chen2020ema/","publishdate":"2020-11-22T00:00:00Z","relpermalink":"/publication/chen2020ema/","section":"publication","summary":"Synthesized speech from articulatory movements can have real-world use for patients with vocal cord disorders, situations requiring silent speech, or in high-noise environments. In this work, we present EMA2S, an end-to-end multimodal articulatory-to-speech system that directly converts articulatory movements to speech signals. We use a neural-network-based vocoder combined with multimodal joint-training, incorporating spectrogram, mel-spectrogram, and deep features. The experimental results confirm that the multimodal approach of EMA2S outperforms the baseline system in terms of both objective evaluation and subjective evaluation metrics. Moreover, results demonstrate that joint mel-spectrogram and deep feature loss training can effectively improve system performance.","tags":null,"title":"EMA2S: An End-to-End Multimodal Articulatory-to-Speech System","type":"publication"},{"authors":["Yu-Wen Chen","Kuo-Hsuan Hung","Shang-Yi Chuang","Jonathan Sherman","Xugang Lu","Yu Tsao"],"categories":null,"content":"","date":1603670400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603670400,"objectID":"551a0bc4cc0baa04df1d7d360f7e5822","permalink":"https://kagaminccino.github.io/publication/chen2020study/","publishdate":"2020-10-26T00:00:00Z","relpermalink":"/publication/chen2020study/","section":"publication","summary":"Although deep-learning algorithms have made great advances on speech enhancement (SE), SE performance is still limited against highly challenging conditions, such as unseen noise types or very low signal-to-noise ratios (SNRs). Given that the mechanisms of vocal articulation are robust or even unaffected by changes in the auditory environment, we propose a novel multimodal audio-articulatory-movement SE model (AAMSE) to improve performance in such challenging conditions. We combine articulatory movement features and audio data for both waveform-mapping-based and spectral-mapping-based SE systems with three fusion strategies. Experimental results confirm that by combining the modalities, AAMSE notably improves the SE performance in both speech quality and intelligibility compared to the audio-only SE baselines. Furthermore, AAMSE shows robust results under very low SNRs and unseen noise type conditions.","tags":null,"title":"A Study of Incorporating Articulatory Movement Information in Speech Enhancement","type":"publication"},{"authors":null,"categories":null,"content":"TMSV (Taiwan Mandarin speech with video) is an audio-visual dataset based on the script of TMHINT (Taiwan Mandarin hearing in noise test). TMSV was recorded by 18 speakers, including 13 males and 5 females, each providing 320 video clips, amounting a total of 5,760 speech utterances. Each utterance in the TMHINT script consists of 10 Chinese characters. The length of each clip in TMSV is approximately 2â€“4 seconds. The video clips were recorded in a recording studio with sufficient light, and the speakers were filmed from the front view at 50 frames per second at a resolution of 1080p. The audio channels were recorded at 48 kHz. The misread labels and actual readed texts for ASR tasks are provided as well. The extracted wav files in the audio folders are downsampled to 16 kHz in mono channel.\n","date":1598745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"1a7f7c933aedcfc78a1e6a3936e7eab6","permalink":"https://kagaminccino.github.io/project/tmsv/","publishdate":"2020-08-30T00:00:00Z","relpermalink":"/project/tmsv/","section":"project","summary":"TMSV is an audio-visual dataset based on the script of TMHINT (Taiwan Mandarin hearing in noise test).","tags":null,"title":"Taiwan Mandarin speech with video (TMSV)","type":"project"},{"authors":["Szu-Wei Fu","Chien-Feng Liao","Tsun-An Hsieh","Kuo-Hsuan Hung","Syu-Siang Wang","Cheng Yu","Heng-Cheng Kuo","Ryandhimas E. Zezario","You-Jin Li","Shang-Yi Chuang","Yen-Ju Lu","Yu Tsao"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"71aa47d44f57ffee23de77d31895824a","permalink":"https://kagaminccino.github.io/publication/fu2020boosting/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/fu2020boosting/","section":"publication","summary":"The Transformer architecture has shown its superior ability than recurrent neural networks on many different natural language processing applications. Therefore, this study applies a modified Transformer on the speech enhancement task. Specifically, the positional encoding may not be necessary and hence is replaced by convolutional layers. To further improve PESQ scores of enhanced speech, the L_1 pre-trained Transformer is fine-tuned by MetricGAN framework. The proposed MetricGAN can be treated as a general post-processing module to further boost interested objective scores. The experiments are conducted using the data sets provided by the organizer of the Deep Noise Suppression (DNS) challenge. Experimental results demonstrate that the proposed system outperforms the challenge baseline in both subjective and objective evaluation with a large margin.","tags":null,"title":"Boosting Objective Scores of Speech Enhancement Model through MetricGAN Post-Processing","type":"publication"},{"authors":["Shang-Yi Chuang","Yu Tsao","Chen-Chou Lo","Hsin-Min Wang"],"categories":null,"content":"","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595548800,"objectID":"f56bfac20406a69ed93154557224dd15","permalink":"https://kagaminccino.github.io/publication/chuang2020lite/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/publication/chuang2020lite/","section":"publication","summary":"Previous studies have confirmed the effectiveness of incorporating visual information into speech enhancement (SE) systems. Despite improved denoising performance, two problems may be encountered when implementing an audio-visual SE (AVSE) system: (1) additional processing costs are incurred to incorporate visual input and (2) the use of face or lip images may cause privacy problems. In this study, we propose a Lite AVSE (LAVSE) system to address these problems. The system includes two visual data compression techniques and removes the visual feature extraction network from the training model, yielding better online computation efficiency. Our experimental results indicate that the proposed LAVSE system can provide notably better performance than an audio-only SE system with a similar number of model parameters. In addition, the experimental results confirm the effectiveness of the two techniques for visual data compression.","tags":null,"title":"Lite Audio-Visual Speech Enhancement","type":"publication"},{"authors":null,"categories":null,"content":"We improved a nonlinear reference shaping controller for manipulators sharing their workspace with humans. The controller is based on the slow and rapid adaptations, which we tried to enhance. After the progress, the slow adaptation can generate movements with smooth endpoint velocity profiles when target position is changed. The rapid adaptation is upgraded as well with respect to not only significantly large external forces but also slight ones. They make the manipulators capable of behaving compliantly to the external forces, and also resuming the motion after the forces are removed, even when the shifts are small. Force detectors are unnecessary in this control system. The validity of the proposed ideas was confirmed via simulations on a planar 4-DOF manipulator.\n","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"8b5f084938a8901441c215a96f881938","permalink":"https://kagaminccino.github.io/project/robotarm/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/project/robotarm/","section":"project","summary":"To realize human behavior on a robot arm based on a nonlinear reference shaping controller in order to create robots which can share a safe working environment with humans.","tags":null,"title":"Smooth and Flexible Movement Control of a Robot Arm","type":"project"},{"authors":null,"categories":null,"content":"The project of the Power Electronics Laboratory, an elective course of dept. of E.E., NTU. Practice of PCB designing and welding.\n","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"b42aeb879c99712de33a146bde9f4fcb","permalink":"https://kagaminccino.github.io/project/flyback/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/project/flyback/","section":"project","summary":"Practice of PCB designing and welding.","tags":null,"title":"Flyback Converter","type":"project"},{"authors":null,"categories":null,"content":"The ME Robot Cup is a traditional annual event of the mechanical engineering department at my university. The whole department, faculty and students, all enjoy the event together. Usually the competitors are junior or senior students, and freshmen and sophomore are the audience. Every year around 20 teams participate the ME Robot Cup with one robot for each. A team usually consists of 4-5 people.\nThe competition that year was to build a pneumatic car which could race over a specific race track with other teams. The race track includes obstacles like hills and some gates the robot needed to pass.\n","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"1e28cca56b227e6f87b56a755ee74106","permalink":"https://kagaminccino.github.io/project/pneumatic/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/project/pneumatic/","section":"project","summary":"The project of Practice of Mechanical Engineering of dept. of M.E., NTU.","tags":null,"title":"ME Robot Cup","type":"project"}]