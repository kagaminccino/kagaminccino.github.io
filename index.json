[{"authors":["admin"],"categories":null,"content":"Shang-Yi Chuang is a graduate student majoring in Computer Science at Cornell Tech, looking for 2022 full-time opportunities in Machine Learning Engineer, Data Scientist, and Software Engineer. She has experience in internaional collaborations in the United States, Japan, and Taiwan.\nShe was a research assistant working on artificial intelligence at the Biomedical Acoustic Signal Processing Lab, Academia Sinica, Taiwan. Her research interests include natural language processing, speech processing, and computer vision. She collaborated with Prof. Yu Tsao and Prof. Hsin-Min Wang on audio-visual multimodal learning and data compression on speech enhancement. She was engaged in a project of a cross-lingual question answering system with Prof. Keh-Yih Su as well.\nBefore she went to Academia Sinica, she was conducting research about humanoid robots with Prof. Tomomichi Sugihara at Motor Intelligence, Osaka University. Her work was to improve the control system of a robot arm in order to create a safer and more comfortable human-robot coworking space.\nHere is her video CV recorded on February 7, 2021 before the enrollment at Cornell Tech! (English subtitles are available.)   ","date":1624060800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1624060800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://kagaminccino.github.io/author/shang-yi-chuang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shang-yi-chuang/","section":"authors","summary":"Shang-Yi Chuang is a graduate student majoring in Computer Science at Cornell Tech, looking for 2022 full-time opportunities in Machine Learning Engineer, Data Scientist, and Software Engineer. She has experience in internaional collaborations in the United States, Japan, and Taiwan.","tags":null,"title":"Shang-Yi Chuang","type":"authors"},{"authors":["Shang-Yi Chuang","Yu Tsao","Hsin-Min Wang"],"categories":null,"content":"","date":1624060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624060800,"objectID":"5b8ade1bc957bea288e03e470ce6d981","permalink":"https://kagaminccino.github.io/publication/chuang2021improved/","publishdate":"2021-06-19T00:00:00Z","relpermalink":"/publication/chuang2021improved/","section":"publication","summary":"Numerous studies have investigated the effectiveness of audio-visual multimodal learning for speech enhancement (AVSE) tasks, seeking a solution that uses visual data as auxiliary and complementary input to reduce the noise of noisy speech signals. Recently, we proposed a lite audio-visual speech enhancement (LAVSE) algorithm. Compared to conventional AVSE systems, LAVSE requires less online computation and moderately solves the user privacy problem on facial data. In this study, we extend LAVSE to improve its ability to address three practical issues often encountered in implementing AVSE systems, namely, the requirement for additional visual data, audio-visual asynchronization, and low-quality visual data. The proposed system is termed improved LAVSE (iLAVSE), which uses a convolutional recurrent neural network architecture as the core AVSE model. We evaluate iLAVSE on the Taiwan Mandarin speech with video dataset. Experimental results confirm that compared to conventional AVSE systems, iLAVSE can effectively overcome the aforementioned three practical issues and can improve enhancement performance. The results also confirm that iLAVSE is suitable for real-world scenarios, where high-quality audio-visual sensors may not always be available.","tags":null,"title":"Improved Lite Audio-Visual Speech Enhancement","type":"publication"},{"authors":null,"categories":null,"content":"Numerous studies have investigated the effectiveness of audio-visual multimodal learning for speech enhancement (AVSE) tasks, seeking a solution that uses visual data as auxiliary and complementary input to reduce the noise of noisy speech signals. Recently, we proposed a lite audio-visual speech enhancement (LAVSE) algorithm. Compared to conventional AVSE systems, LAVSE requires less online computation and moderately solves the user privacy problem on facial data. In this study, we extend LAVSE to improve its ability to address three practical issues often encountered in implementing AVSE systems, namely, the requirement for additional visual data, audio-visual asynchronization, and low-quality visual data. The proposed system is termed improved LAVSE (iLAVSE), which uses a convolutional recurrent neural network architecture as the core AVSE model. We evaluate iLAVSE on the Taiwan Mandarin speech with video dataset. Experimental results confirm that compared to conventional AVSE systems, iLAVSE can effectively overcome the aforementioned three practical issues and can improve enhancement performance. The results also confirm that iLAVSE is suitable for real-world scenarios, where high-quality audio-visual sensors may not always be available.\n","date":1624060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624060800,"objectID":"e4ef1b55cb006c789e7312099efad4a7","permalink":"https://kagaminccino.github.io/project/ilavse/","publishdate":"2021-06-19T00:00:00Z","relpermalink":"/project/ilavse/","section":"project","summary":"iLAVSE is a deep-learning-based audio-visual project that addresses three practical issues often encountered in implementing AVSE systems, including the requirement for additional visual data, audio-visual asynchronization, and low-quality visual data.","tags":null,"title":"Improved Lite Audio-Visual Speech Enhancement (iLAVSE)","type":"project"},{"authors":["Yu-Wen Chen","Kuo-Hsuan Hung","Shang-Yi Chuang","Jonathan Sherman","Xugang Lu","Yu Tsao"],"categories":null,"content":"","date":1620172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620172800,"objectID":"077e1c28c4ebeab37b0d63853b10efa2","permalink":"https://kagaminccino.github.io/publication/chen2021study/","publishdate":"2021-05-05T00:00:00Z","relpermalink":"/publication/chen2021study/","section":"publication","summary":"Although deep learning algorithms are widely used for improving speech enhancement (SE) performance, the performance remains limited under highly challenging conditions, such as unseen noise or noise signals having low signal-to-noise ratios (SNRs). This study provides a pilot investigation on a novel multimodal audio-articulatory-movement SE (AAMSE) model to enhance SE performance under such challenging conditions. Articulatory movement features and acoustic signals were used as inputs to waveform-mapping-based and spectral-mapping-based SE systems with three fusion strategies. In addition, an ablation study was conducted to evaluate SE performance using a limited number of articulatory movement sensors. Experimental results confirm that, by combining the modalities, the AAMSE model notably improves the SE performance in terms of speech quality and intelligibility, as compared to conventional audio-only SE baselines.","tags":null,"title":"A Study of Incorporating Articulatory Movement Information in Speech Enhancement","type":"publication"},{"authors":["Yu-Wen Chen","Kuo-Hsuan Hung","Shang-Yi Chuang","Jonathan Sherman","Wen-Chin Huang","Xugang Lu","Yu Tsao"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"ff2dbb805b12c4cbc1b9b4a0eb286ea8","permalink":"https://kagaminccino.github.io/publication/chen2021ema2s/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/publication/chen2021ema2s/","section":"publication","summary":"Synthesized speech from articulatory movements can have real-world use for patients with vocal cord disorders, situations requiring silent speech, or in high-noise environments. In this work, we present EMA2S, an end-to-end multimodal articulatory-to-speech system that directly converts articulatory movements to speech signals. We use a neural-network-based vocoder combined with multimodal joint-training, incorporating spectrogram, mel-spectrogram, and deep features. The experimental results confirm that the multimodal approach of EMA2S outperforms the baseline system in terms of both objective evaluation and subjective evaluation metrics. Moreover, results demonstrate that joint mel-spectrogram and deep feature loss training can effectively improve system performance.","tags":null,"title":"EMA2S: An End-to-End Multimodal Articulatory-to-Speech System","type":"publication"},{"authors":null,"categories":null,"content":"TMSV (Taiwan Mandarin Speech with Video) is an audio-visual dataset based on the script of TMHINT (Taiwan Mandarin hearing in noise test). TMSV was recorded by 18 speakers, including 13 males and 5 females, each providing 320 video clips, amounting a total of 5,760 speech utterances. Each utterance in the TMHINT script consists of 10 Chinese characters. The length of each clip in TMSV is approximately 2â€“4 seconds. The video clips were recorded in a recording studio with sufficient light, and the speakers were filmed from the front view at 50 frames per second at a resolution of 1080p. The audio channels were recorded at 48 kHz. The misread labels and actual readed texts for ASR tasks are provided as well. The extracted wav files in the audio folders are downsampled to 16 kHz in mono channel.\n","date":1598745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"1a7f7c933aedcfc78a1e6a3936e7eab6","permalink":"https://kagaminccino.github.io/project/tmsv/","publishdate":"2020-08-30T00:00:00Z","relpermalink":"/project/tmsv/","section":"project","summary":"TMSV is an audio-visual dataset based on the script of TMHINT (Taiwan Mandarin hearing in noise test).","tags":null,"title":"Taiwan Mandarin Speech with Video (TMSV)","type":"project"},{"authors":["Szu-Wei Fu","Chien-Feng Liao","Tsun-An Hsieh","Kuo-Hsuan Hung","Syu-Siang Wang","Cheng Yu","Heng-Cheng Kuo","Ryandhimas E. Zezario","You-Jin Li","Shang-Yi Chuang","Yen-Ju Lu","Yu Tsao"],"categories":null,"content":"","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596240000,"objectID":"71aa47d44f57ffee23de77d31895824a","permalink":"https://kagaminccino.github.io/publication/fu2020boosting/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/fu2020boosting/","section":"publication","summary":"The Transformer architecture has shown its superior ability than recurrent neural networks on many different natural language processing applications. Therefore, this study applies a modified Transformer on the speech enhancement task. Specifically, the positional encoding may not be necessary and hence is replaced by convolutional layers. To further improve PESQ scores of enhanced speech, the L1 pre-trained Transformer is fine-tuned by MetricGAN framework. The proposed MetricGAN can be treated as a general post-processing module to further boost interested objective scores. The experiments are conducted using the data sets provided by the organizer of the Deep Noise Suppression (DNS) challenge. Experimental results demonstrate that the proposed system outperforms the challenge baseline in both subjective and objective evaluation with a large margin.","tags":null,"title":"Boosting Objective Scores of Speech Enhancement Model through MetricGAN Post-Processing","type":"publication"},{"authors":["Shang-Yi Chuang","Yu Tsao","Chen-Chou Lo","Hsin-Min Wang"],"categories":null,"content":"","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595548800,"objectID":"f56bfac20406a69ed93154557224dd15","permalink":"https://kagaminccino.github.io/publication/chuang2020lite/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/publication/chuang2020lite/","section":"publication","summary":"Previous studies have confirmed the effectiveness of incorporating visual information into speech enhancement (SE) systems. Despite improved denoising performance, two problems may be encountered when implementing an audio-visual SE (AVSE) system: (1) additional processing costs are incurred to incorporate visual input and (2) the use of face or lip images may cause privacy problems. In this study, we propose a Lite AVSE (LAVSE) system to address these problems. The system includes two visual data compression techniques and removes the visual feature extraction network from the training model, yielding better online computation efficiency. Our experimental results indicate that the proposed LAVSE system can provide notably better performance than an audio-only SE system with a similar number of model parameters. In addition, the experimental results confirm the effectiveness of the two techniques for visual data compression.","tags":null,"title":"Lite Audio-Visual Speech Enhancement","type":"publication"},{"authors":null,"categories":null,"content":"Previous studies have confirmed the effectiveness of incorporating visual information into speech enhancement (SE) systems. Despite improved denoising performance, two problems may be encountered when implementing an audio-visual SE (AVSE) system: (1) additional processing costs are incurred to incorporate visual input and (2) the use of face or lip images may cause privacy problems. In this study, we propose a Lite AVSE (LAVSE) system to address these problems. The system includes two visual data compression techniques and removes the visual feature extraction network from the training model, yielding better online computation efficiency. Our experimental results indicate that the proposed LAVSE system can provide notably better performance than an audio-only SE system with a similar number of model parameters. In addition, the experimental results confirm the effectiveness of the two techniques for visual data compression.\n","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595548800,"objectID":"35441e737ea38709c6be66b96f2dd9c3","permalink":"https://kagaminccino.github.io/project/lavse/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/project/lavse/","section":"project","summary":"LAVSE is a deep-learning-based audio-visual project that addresses additional processing costs and privacy problems.","tags":null,"title":"Lite Audio-Visual Speech Enhancement (LAVSE)","type":"project"},{"authors":null,"categories":null,"content":"We improved a nonlinear reference shaping controller for manipulators sharing their workspace with humans. The controller is based on the slow and rapid adaptations, which we tried to enhance. After the progress, the slow adaptation can generate movements with smooth endpoint velocity profiles when target position is changed. The rapid adaptation is upgraded as well with respect to not only significantly large external forces but also slight ones. They make the manipulators capable of behaving compliantly to the external forces, and also resuming the motion after the forces are removed, even when the shifts are small. Force detectors are unnecessary in this control system. The validity of the proposed ideas was confirmed via simulations on a planar 4-DOF manipulator.\n","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"8b5f084938a8901441c215a96f881938","permalink":"https://kagaminccino.github.io/project/robotarm/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/project/robotarm/","section":"project","summary":"Realizing human behavior on a robot arm based on a nonlinear reference shaping controller in order to create robots which can share a safe working environment with humans.","tags":null,"title":"Smooth and Flexible Movement Control of a Robot Arm","type":"project"},{"authors":null,"categories":null,"content":"The project of the Power Electronics Laboratory, an elective course at the Department of Electrical Engineering, NTU. Practice of PCB designing and welding.\n","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"b42aeb879c99712de33a146bde9f4fcb","permalink":"https://kagaminccino.github.io/project/flyback/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/project/flyback/","section":"project","summary":"Practice of PCB designing and welding.","tags":null,"title":"Flyback Converter","type":"project"},{"authors":null,"categories":null,"content":"The ME Robot Cup is a traditional annual event at the Department of Mechanical Engineering, NTU. The whole department, faculty and students, all enjoy the event together. Usually the competitors are junior or senior students, and freshmen and sophomore are the audience. Every year around 20 teams participate the ME Robot Cup with one robot for each. A team usually consists of 4-5 people.\nThe competition that year was to build a pneumatic car which could race over a specific race track with other teams. The race track includes obstacles like hills and some gates the robot needed to pass.\n","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"1e28cca56b227e6f87b56a755ee74106","permalink":"https://kagaminccino.github.io/project/pneumatic/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/project/pneumatic/","section":"project","summary":"The ME Robot Cup is a traditional annual event at the Department of Mechanical Engineering, NTU.","tags":null,"title":"ME Robot Cup","type":"project"}]